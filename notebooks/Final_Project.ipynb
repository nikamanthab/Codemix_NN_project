{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "Bnz-TXhOR7oP"
   },
   "outputs": [],
   "source": [
    "# place necessary imports here\n",
    "import random\n",
    "import pandas as pd\n",
    "import wandb\n",
    "import torch\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from pymagnitude import Magnitude\n",
    "import os\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch import optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import accuracy_score, f1_score, recall_score, precision_score\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "from nltk import word_tokenize\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from datasets import list_datasets, load_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7arNSE2gYW1l"
   },
   "source": [
    "# Evaluation of Language Models : Hands-on\n",
    "In this hands-on you will learn to perform sentence classification/sentiment analysis on a dataset using different Language models and compare their performance.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gGWfP2WGRzFg"
   },
   "source": [
    "# Dataset description\n",
    "The datasets we will be using are mutlilingual or codemixed datasets. Each of the datasets are English mixed with a Dravidian language. In the sense that the dataset contains sample sentences which are either written completely in a dravidian language script or using the English alphabet and may contain English words. We will be using 5 datasets: 2 datasets in Tamil-English, 2 datasets in Kannada-English and 1 dataset in Malayalam-English. The dataset has class imbalance problems as a result of which we will compare the performance of the Language models using F1 scores. The samples are mostly taken from Youtube comments sections and hence correspond to real world conversations. More information about the datasets can be seen from the sources listed below. <br>\n",
    "Sources: <br>\n",
    "<ol>\n",
    "<li>https://huggingface.co/datasets/tamilmixsentiment\n",
    "<li>https://huggingface.co/datasets/offenseval_dravidian\n",
    "<li>https://huggingface.co/datasets/kan_hope\n",
    "</ol>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "BZbUkeJ4UJ52"
   },
   "outputs": [],
   "source": [
    "def get_data(tag):\n",
    "    dataset = load_dataset(tag[0], tag[1])\n",
    "    train_df = pd.DataFrame()\n",
    "    val_df = pd.DataFrame()\n",
    "\n",
    "    train_df['text'] = dataset['train']['text']\n",
    "    train_df['label'] = dataset['train']['label']\n",
    "    \n",
    "    if tag[0] == 'kan_hope':\n",
    "        val_df['text'] = dataset['test']['text']\n",
    "        val_df['label'] = dataset['test']['label']\n",
    "    else:\n",
    "        val_df['text'] = dataset['validation']['text']\n",
    "        val_df['label'] = dataset['validation']['label']\n",
    "\n",
    "    if tag[0] == 'offenseval_dravidian' and tag[2] == 'ml':\n",
    "        train_df['label'].replace(5, 4, inplace=True)\n",
    "        val_df['label'].replace(5, 4, inplace=True)\n",
    "    \n",
    "    return train_df[:], val_df[:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "f9FCsePridJ_"
   },
   "outputs": [],
   "source": [
    "tags = [\n",
    "        ('tamilmixsentiment', None, 'ta'), \n",
    "        ('offenseval_dravidian', 'tamil', 'ta'), \n",
    "        ('offenseval_dravidian', 'malayalam', 'ml'),\n",
    "        ('offenseval_dravidian', 'kannada', 'kn'),\n",
    "        ('kan_hope', None, 'kn')\n",
    "        ]\n",
    "\n",
    "tag_dict = {\n",
    "    'tamilmixsentiment': tags[0],\n",
    "    'offenseval_dravidian_ta': tags[1],\n",
    "    'offenseval_dravidian_ml': tags[2],\n",
    "    'offenseval_dravidian_kn': tags[3],\n",
    "    'kan_hope': tags[4]\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Samples from datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 304
    },
    "id": "wjmzr_1BUHxQ",
    "outputId": "ca6438b7-3e2d-49c2-d260-09d2eca6b10b"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default\n",
      "Reusing dataset tamilmixsentiment (/users/PAS0536/deepaksuresh94/.cache/huggingface/datasets/tamilmixsentiment/default/0.0.0/887420eecaf868ac6c10990649e49d10467e4cd4dffb98a6f20e4fe7c58df390)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2b4e52324013408dad5eace938cadd00",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                               text  label\n",
      "0          Trailer late ah parthavanga like podunga      0\n",
      "1        Move pathutu vanthu trailer pakurvnga yaru      0\n",
      "2          Puthupetai dhanush  ah yarellam pathinga      0\n",
      "3  Dhanush oda character ,puthu sa erukay , mass ta      0\n",
      "4  vera level ippa pesungada mokka nu thalaivaaaaaa      0\n",
      "\u001b[1mDataset: tamilmixsentiment, Language: ta\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset offenseval_dravidian (/users/PAS0536/deepaksuresh94/.cache/huggingface/datasets/offenseval_dravidian/tamil/1.0.0/caf62757ff7f5922e043f21abf68745096b24007c4b79d5b2344ea3a7238563f)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3dbc321a846949c1836b0ee31101dda3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                text  label\n",
      "0                  movie vara level la Erika poguthu      0\n",
      "1  I love Ajith Kumar Vivegam movie inki mjy bht ...      5\n",
      "2          Padam nalla comedy padama irukum polaye..      0\n",
      "3  karthick subburaj anne .... intha padam vetri ...      0\n",
      "4  கவுண்டர் தேவர்.சார்பாக வெற்றி பெற வாழ்த்துக்கள் 🦁      0\n",
      "\u001b[1mDataset: offenseval_dravidian, Language: ta\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset offenseval_dravidian (/users/PAS0536/deepaksuresh94/.cache/huggingface/datasets/offenseval_dravidian/malayalam/1.0.0/caf62757ff7f5922e043f21abf68745096b24007c4b79d5b2344ea3a7238563f)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d531b1c32909448a8d0352dab375847b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                text  label\n",
      "0  പലദേശം. പല ഭാഷ ഒരേ ഒരു രാജാവ്  അല്ലാതെ  സ്വന്ത...      0\n",
      "1  ഈ ഓണം ഏട്ടനും പിള്ളേർക്ക് ഉള്ളതാണ് എന്ന് ഉള്ളവ...      0\n",
      "2  ആരണ്ട ആരണ്ട തലുണ്ടാകാണാ ആരണ്ട ഞാൻ ആണ്ട ഞാൻ ആണ്...      0\n",
      "3          Sushin syam  Shaiju khalid  Midhun manual      0\n",
      "4                          J A K E S.   B EJ O Y !!!      0\n",
      "\u001b[1mDataset: offenseval_dravidian, Language: ml\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset offenseval_dravidian (/users/PAS0536/deepaksuresh94/.cache/huggingface/datasets/offenseval_dravidian/kannada/1.0.0/caf62757ff7f5922e043f21abf68745096b24007c4b79d5b2344ea3a7238563f)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "edd1f060e2c64ad38a96fad17cd5e5da",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                text  label\n",
      "0  Tik tok alli jagala madtidralla adra baggenu o...      0\n",
      "1                            Anyone from kerala here      5\n",
      "2                          Movie rerelease madi plss      0\n",
      "3  Amazon prime alli bittidira....yella manele no...      0\n",
      "4  Guru sure news nanu tik tok dawn lod madeda ya...      0\n",
      "\u001b[1mDataset: offenseval_dravidian, Language: kn\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default\n",
      "Reusing dataset kan_hope (/users/PAS0536/deepaksuresh94/.cache/huggingface/datasets/kan_hope/default/0.0.0/3ded8b2dea549473aa58db03694800c10b5b5e29c3206385f5044dcb6338ebc3)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f2a073d22d4c4865a30599439ad21e9c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                text  label\n",
      "0                         Valle story iratte maathra      0\n",
      "1                             @10 R report madi avna      0\n",
      "2              ಕಿಚ್ಚನ ಹುಡುಗ್ರು ವತಿಯಿಂದ  all the best      0\n",
      "3  Diya thumba chennagide ondu olle prayathna mov...      1\n",
      "4                         ಇದು ಚರಿತ್ರೆ ಸೃಷ್ಟಿಸೋ ಅವತಾರ      1\n",
      "\u001b[1mDataset: kan_hope, Language: kn\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "BOLD = '\\033[1m'\n",
    "END = '\\033[0m'\n",
    "for tag in tags:\n",
    "    train_df, val_df = get_data(tag)\n",
    "    print(train_df.head())\n",
    "    print('{}Dataset: {}, Language: {}{}'.format(BOLD,tag[0],tag[2],END))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qDOw3XNrdKhe"
   },
   "source": [
    "# Feed-Forward Network - Fine Tuning \n",
    "Training of these Language models from scratch on the given datasets takes considerable amount of time. In order to save time, one can opt to fine tune a pre-trained language model to train on new unseen data and unseen labels. One method of Fine tuning involves using a layer of Feed-Forward network which takes the embedding as the input and outputs the target labels. The Feed forward network is essentially a Multi-Layer Perceptron Layer. We will implement this using Pytorch.\n",
    "\n",
    "More information about the concept and merits of Fine Tuning can be found here: https://www.analyticsvidhya.com/blog/2020/07/transfer-learning-for-nlp-fine-tuning-bert-for-text-classification/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "HWbSTaaKU4Dd"
   },
   "outputs": [],
   "source": [
    "class LinearNetwork_2Layer(nn.Module):\n",
    "    def __init__(self, input_size, output_size,hn,p):\n",
    "        super().__init__()\n",
    "        # define layers\n",
    "        self.fc1 = nn.Linear(in_features=input_size,out_features=hn)\n",
    "        self.fc2 = nn.Linear(in_features=hn,out_features=output_size)\n",
    "        self.dropout = nn.Dropout(p)\n",
    "    # define forward function\n",
    "    def forward(self, t):\n",
    "        # fc 1\n",
    "        t=self.fc1(t)\n",
    "        t=F.relu(t)\n",
    "        t = self.dropout(t)\n",
    "        # fc 2\n",
    "        t=self.fc2(t)\n",
    "        # don't need softmax here since we'll use cross-entropy as activation.\n",
    "        return t\n",
    "\n",
    "class LinearNetwork_4Layer(nn.Module):\n",
    "    def __init__(self, input_size, output_size,hn,p):\n",
    "        hn1, hn2, hn3 = hn\n",
    "        super().__init__()\n",
    "        # define layers\n",
    "        self.fc1 = nn.Linear(in_features=input_size,out_features=hn1)\n",
    "        self.fc2 = nn.Linear(in_features=hn1,out_features=hn2)\n",
    "        self.fc3 = nn.Linear(in_features=hn2,out_features=hn3)\n",
    "        self.fc4 = nn.Linear(in_features=hn3,out_features=output_size)\n",
    "        self.dropout = nn.Dropout(p)\n",
    "    # define forward function\n",
    "    def forward(self, t):\n",
    "        # fc 1\n",
    "        t=self.fc1(t)\n",
    "        t=F.relu(t)\n",
    "        t = self.dropout(t)\n",
    "        # fc 2\n",
    "        t=self.fc2(t)\n",
    "        t=F.relu(t)\n",
    "        t = self.dropout(t)\n",
    "         # fc 3\n",
    "        t=self.fc3(t)\n",
    "        t=F.relu(t)\n",
    "        t = self.dropout(t)\n",
    "         # fc 4\n",
    "        t=self.fc4(t)\n",
    "        # don't need softmax here since we'll use cross-entropy as activation.\n",
    "        return t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LUyD0Dpqenq5"
   },
   "source": [
    "# Language models: Data embedding and Data loading\n",
    "The Language models that we will evaluate are the following:\n",
    "<ol>\n",
    "<li>TF-IDF Vectorizer\n",
    "<li>Count Vectorizer\n",
    "<li>BERT - https://arxiv.org/pdf/1810.04805.pdf\n",
    "<li>Word2vec - https://arxiv.org/pdf/1301.3781.pdf\n",
    "<li>GloVe - https://nlp.stanford.edu/pubs/glove.pdf\n",
    "<li>Fasttext - https://arxiv.org/pdf/1607.04606.pdf\n",
    "</ol>\n",
    "More information about the models can be found in the referenced papers given next to each of them.\n",
    "\n",
    "The function get_embedding() outputs the embedding of the sentences in our datasets using the pretrained models. We will make use of pre-trained models followed by a fine-tuning layer. The pymagnitude library is used to load the pretrained weights and get the embeddings for Word2vec, GloVe and Fasttext. As the get_embedding process takes considerable amount of time and is going to remain constant for each of the language models used, we have saved the embeddings for train and test data of the datasets and can be loaded directly using load_embedding(). The get_embedding() provided here serves as a reference.  \n",
    "\n",
    "CodemixDataset is a custom Dataset class implementing 3 functions: init, len and getitem. The init function run during the instantiating of the Dataset object calls get_embedding() or load_embedding() function to prepare the embeddings on our Dataset that we will be using. The len function returns the number of samples in our Dataset. The getitem function loads and returns a sample from the dataset at the given index idx. The CodemixDataset retrieves our dataset’s features and labels one sample at a time. We will load CodemixDataset into the Dataloader to iterate through our dataset. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-x5W7buXU2eS"
   },
   "outputs": [],
   "source": [
    "def get_embedding(text, model, model_type):\n",
    "    if model_type in ['TfidfVectorizer', 'CountVectorizer']:\n",
    "        return model.transform(text).toarray()\n",
    "    elif model_type == 'BERT':\n",
    "        print(\"BERT model embedding...\")\n",
    "        return model.encode(text, batch_size=8, show_progress_bar=True)\n",
    "    elif model_type == 'magnitude':\n",
    "        vectors = []\n",
    "        for sentence in tqdm(text):\n",
    "            vectors.append(np.average(model.query(word_tokenize(sentence)), axis=0))\n",
    "        return vectors  \n",
    "\n",
    "def load_embedding(dataset_name, model_name, mode):\n",
    "    if model_name in ['TfidfVectorizer', 'CountVectorizer','BERT']:\n",
    "        filename = 'embeddings/'+dataset_name+'/'+model_name+'/'+mode+'Embedding_'+dataset_name+'_'+model_name+'.pt'\n",
    "    else:\n",
    "        filename = 'embeddings/'+dataset_name+'/'+model_name+'/'+mode+'Embedding_'+dataset_name+'_magnitude.pt'\n",
    "    embedding = torch.load(filename)\n",
    "    return embedding\n",
    "    \n",
    "class CodemixDataset(Dataset):\n",
    "    def __init__(self, df, encoder_model, encoding_type,dataset_name,model_name,mode,load=True):\n",
    "        self.df = df\n",
    "        print(\"Creating Embedding... Will take some time...\")\n",
    "        if load:\n",
    "            self.embedding = load_embedding(dataset_name,model_name,mode)\n",
    "        else:            \n",
    "            self.embedding = get_embedding(list(self.df['text']), \\\n",
    "                                          encoder_model, encoding_type)\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "        text_embedding = self.embedding[idx]\n",
    "        label = self.df.iloc[idx]['label']\n",
    "        sample = {\n",
    "            'text': text_embedding,\n",
    "            'label': label\n",
    "        }\n",
    "        return sample\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7f_bakjLfdyb"
   },
   "source": [
    "# Training\n",
    "Training is primarily training the fine tuning layer which is the MLP implemented using Pytorch. The final run consists of \n",
    "<ol>\n",
    "<li>Loading the dataset \n",
    "<li>Getting the embeddings\n",
    "<li>Train the MLP using Train data\n",
    "<li>Predict the labels of Test data\n",
    "<li>Measure F1 score on Test data\n",
    "</ol>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "g0ST4OkfVBbQ"
   },
   "outputs": [],
   "source": [
    "def train(model_architecture, encoder_model, encoding_type, train_df, test_df, epochs, \\\n",
    "          learning_rate, batch_size, log, device, hiddenNodes, dropout, train_loader, test_loader):\n",
    "\n",
    "    encoding = get_embedding([train_df.iloc[0]['text']], encoder_model, encoding_type)[0]\n",
    "    input_size = encoding.shape\n",
    "    model = model_architecture(input_size[0], len(train_df['label'].unique()),hiddenNodes,dropout).to(device)\n",
    "\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "    print(\"model architecture:\")\n",
    "    print(model)\n",
    "\n",
    "    print(\"train_df classes\")\n",
    "    print(train_df['label'].value_counts())\n",
    "\n",
    "    model.train()\n",
    "    for epoch in range(epochs):\n",
    "        output_labels = torch.Tensor().to(device)\n",
    "        true_labels = torch.Tensor().to(device)\n",
    "        total_loss = 0\n",
    "        for data in tqdm(train_loader):\n",
    "            text, labels = data['text'], data['label']\n",
    "            inputs = text.to(device)\n",
    "            labels = labels.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs.type(torch.float))\n",
    "            predicted_labels = torch.argmax(outputs, dim=1)\n",
    "            output_labels = torch.cat((output_labels, predicted_labels), 0)\n",
    "            true_labels = torch.cat((true_labels, labels), 0)\n",
    "            loss = criterion(outputs, labels.to(device))\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "        tr_accuracy, tr_f1, tr_recall, tr_precision = get_scores(true_labels, output_labels)\n",
    "        net_train_loss = total_loss/(len(train_loader)*batch_size)\n",
    "        print('Epoch: {}\\t Train Loss: {:.4f} \\t Train F1:{:.2f}'.format(epoch, net_train_loss, tr_f1), end='\\t')\n",
    "        \n",
    "        output_labels = torch.Tensor().to(device)\n",
    "        true_labels = torch.Tensor().to(device)\n",
    "        total_loss = 0\n",
    "        for i, data in enumerate(test_loader, 0):\n",
    "            text, labels = data['text'], data['label']\n",
    "            \n",
    "            inputs = text.to(device)\n",
    "            labels = labels.to(device)\n",
    "            \n",
    "            outputs = model(inputs.type(torch.float))\n",
    "            predicted_labels = torch.argmax(outputs, dim=1)\n",
    "            output_labels = torch.cat((output_labels, predicted_labels), 0)\n",
    "            true_labels = torch.cat((true_labels, labels), 0)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            total_loss += loss.item()\n",
    "        ts_accuracy, ts_f1, ts_recall, ts_precision = get_scores(true_labels, output_labels)\n",
    "        net_test_loss = total_loss/(len(train_loader)*batch_size)\n",
    "        print('Test Loss: {:.4f} \\t Test F1:{:.2f}'.format(net_test_loss, ts_f1))\n",
    "        \n",
    "        if log != None:\n",
    "            log({\n",
    "                    \"train accuracy\": tr_accuracy,\n",
    "                    \"train f1\": tr_f1,\n",
    "                    \"train recall\": tr_recall,\n",
    "                    \"train precision\": tr_precision,\n",
    "                    \n",
    "                    \"test accuarcy\": ts_accuracy,\n",
    "                    \"test f1\": ts_f1,\n",
    "                    \"test recall\": ts_recall,\n",
    "                    \"test precision\": ts_precision,\n",
    "                    \n",
    "                    \"train loss\": net_train_loss,\n",
    "                    \"test loss\": net_test_loss\n",
    "                    })\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XPVF7saGU9XO"
   },
   "outputs": [],
   "source": [
    "def get_scores(y_true, y_pred):\n",
    "    if torch.cuda.is_available():\n",
    "        accuracy = accuracy_score(y_true.cpu(), y_pred.cpu())\n",
    "        f1 = f1_score(y_true.cpu(), y_pred.cpu(), average='weighted')\n",
    "        recall = recall_score(y_true.cpu(), y_pred.cpu(), average='weighted')\n",
    "        precision = precision_score(y_true.cpu(), y_pred.cpu(), average='weighted')\n",
    "    else:\n",
    "        accuracy = accuracy_score(y_true, y_pred)\n",
    "        f1 = f1_score(y_true, y_pred, average='weighted')\n",
    "        recall = recall_score(y_true, y_pred, average='weighted')\n",
    "        precision = precision_score(y_true, y_pred, average='weighted')\n",
    "    return accuracy, f1, recall, precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_dict = {\n",
    "    'tamilmixsentiment': {\n",
    "        'CountVectorizer': {\n",
    "            'max_features': 5000,\n",
    "            'analyzer': 'word',\n",
    "            'range': (2,5),\n",
    "            'mlp_layers': (512,256,128),\n",
    "            'mlp_dropout': 0.25\n",
    "        },\n",
    "        'TfidfVectorizer': {\n",
    "            'max_features': 2000,\n",
    "            'analyzer': 'char',\n",
    "            'range': (2,5),\n",
    "            'mlp_layers': (512,256,128),\n",
    "            'mlp_dropout': 0.25\n",
    "        },\n",
    "        'BERT': {\n",
    "            'mlp_layers': 1024,\n",
    "            'mlp_dropout': 0\n",
    "        },\n",
    "        'glove': {\n",
    "            'mlp_layers': (1024,512,256),\n",
    "            'mlp_dropout': 0\n",
    "        },\n",
    "        'word2vec': {\n",
    "            'mlp_layers': 2048,\n",
    "            'mlp_dropout': 0\n",
    "        },\n",
    "        'fasttext': {\n",
    "            'mlp_layers': 2048,\n",
    "            'mlp_dropout': 0\n",
    "        },\n",
    "    },\n",
    "    'offenseval_dravidian_ta': {\n",
    "        'CountVectorizer': {\n",
    "            'max_features': 10000,\n",
    "            'analyzer': 'char',\n",
    "            'range': (1,3),\n",
    "            'mlp_layers': 2048,\n",
    "            'mlp_dropout': 0\n",
    "        },\n",
    "        'TfidfVectorizer': {\n",
    "            'max_features': 10000,\n",
    "            'analyzer': 'char',\n",
    "            'range': (1,3),\n",
    "            'mlp_layers': 2048,\n",
    "            'mlp_dropout': 0\n",
    "        },\n",
    "        'BERT': {\n",
    "            'mlp_layers': 1024,\n",
    "            'mlp_dropout': 0\n",
    "        },\n",
    "        'glove': {\n",
    "            'mlp_layers': 1024,\n",
    "            'mlp_dropout': 0\n",
    "        },\n",
    "        'word2vec': {\n",
    "            'mlp_layers': 1024,\n",
    "            'mlp_dropout': 0\n",
    "        },\n",
    "        'fasttext': {\n",
    "            'mlp_layers': 1024,\n",
    "            'mlp_dropout': 0\n",
    "        },\n",
    "    },\n",
    "    'offenseval_dravidian_ml': {\n",
    "        'CountVectorizer': {\n",
    "            'max_features': 5000,\n",
    "            'analyzer': 'word',\n",
    "            'range': (2,5),\n",
    "            'mlp_layers': (1024,512,256),\n",
    "            'mlp_dropout': 0.25\n",
    "        },\n",
    "        'TfidfVectorizer': {\n",
    "            'max_features': 10000,\n",
    "            'analyzer': 'word',\n",
    "            'range': (2,5),\n",
    "            'mlp_layers': 2048,\n",
    "            'mlp_dropout': 0\n",
    "        },\n",
    "        'BERT': {\n",
    "            'mlp_layers': (1024,512,256),\n",
    "            'mlp_dropout': 0\n",
    "        },\n",
    "        'glove': {\n",
    "            'mlp_layers': (512,256,128),\n",
    "            'mlp_dropout': 0\n",
    "        },\n",
    "        'word2vec': {\n",
    "            'mlp_layers': 2048,\n",
    "            'mlp_dropout': 0\n",
    "        },\n",
    "        'fasttext': {\n",
    "            'mlp_layers': 1024,\n",
    "            'mlp_dropout': 0\n",
    "        },\n",
    "    },\n",
    "    'offenseval_dravidian_kn': {\n",
    "        'CountVectorizer': {\n",
    "            'max_features': 2000,\n",
    "            'analyzer': 'char',\n",
    "            'range': (2,5),\n",
    "            'mlp_layers': 2048,\n",
    "            'mlp_dropout': 0\n",
    "        },\n",
    "        'TfidfVectorizer': {\n",
    "            'max_features': 5000,\n",
    "            'analyzer': 'word',\n",
    "            'range': (2,5),\n",
    "            'mlp_layers': (1024,512,256),\n",
    "            'mlp_dropout': 0.25\n",
    "        },\n",
    "        'BERT': {\n",
    "            'mlp_layers': 2048,\n",
    "            'mlp_dropout': 0.25\n",
    "        },\n",
    "        'glove': {\n",
    "            'mlp_layers': 1024,\n",
    "            'mlp_dropout': 0.25\n",
    "        },\n",
    "        'word2vec': {\n",
    "            'mlp_layers': (1024,512,256),\n",
    "            'mlp_dropout': 0.25\n",
    "        },\n",
    "        'fasttext': {\n",
    "            'mlp_layers': (1024,512,256),\n",
    "            'mlp_dropout': 0.25\n",
    "        },\n",
    "    },\n",
    "    'kan_hope': {\n",
    "        'CountVectorizer': {\n",
    "            'max_features': 1000,\n",
    "            'analyzer': 'char',\n",
    "            'range': (1,3),\n",
    "            'mlp_layers': 2048,\n",
    "            'mlp_dropout': 0.25\n",
    "        },\n",
    "        'TfidfVectorizer': {\n",
    "            'max_features': 10000,\n",
    "            'analyzer': 'word',\n",
    "            'range': (2,5),\n",
    "            'mlp_layers': (1024,512,256),\n",
    "            'mlp_dropout': 0\n",
    "        },\n",
    "        'BERT': {\n",
    "            'mlp_layers': (1024,512,256),\n",
    "            'mlp_dropout': 0\n",
    "        },\n",
    "        'glove': {\n",
    "            'mlp_layers': 2048,\n",
    "            'mlp_dropout': 0\n",
    "        },\n",
    "        'word2vec': {\n",
    "            'mlp_layers': 1024,\n",
    "            'mlp_dropout': 0.25\n",
    "        },\n",
    "        'fasttext': {\n",
    "            'mlp_layers': 1024,\n",
    "            'mlp_dropout': 0.25\n",
    "        }\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3m620S-0VJMx"
   },
   "outputs": [],
   "source": [
    "EPOCHS = 50\n",
    "ETA = 0.001\n",
    "BATCH_SIZE = 64\n",
    "tdevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "weight_path=''\n",
    "\n",
    "load_embedding = True\n",
    "for dataset_name, tag in tag_dict.items():\n",
    "    train_df, val_df = get_data(tag)\n",
    "    \n",
    "    for model_name, config in config_dict[dataset_name].items():\n",
    "        if !load_embedding:\n",
    "            #change embedding model\n",
    "            encoder_model = None\n",
    "            if model_type == 'magnitude':\n",
    "                english_path = os.path.join(arguments['weights_path'], 'english', model_name+'.magnitude')\n",
    "                dravidian_path = os.path.join(arguments['weights_path'], 'dravidian', tag[2], model_name+'.magnitude')\n",
    "\n",
    "                english_model = Magnitude(english_path)\n",
    "                dravidian_model = Magnitude(dravidian_path)\n",
    "                encoder_model = Magnitude(english_model, dravidian_model, devices=[0,1])\n",
    "            elif model_type == 'BERT':\n",
    "                encoder_model = SentenceTransformer('distiluse-base-multilingual-cased')\n",
    "            elif model_type == 'TfidfVectorizer':\n",
    "                encoder_model = TfidfVectorizer(analyzer=arguments['analyzer'], ngram_range=arguments['range'], max_features=arguments['max_features'])\n",
    "                encoder_model.fit(train_df['text'])\n",
    "            elif model_type == 'CountVectorizer':\n",
    "                encoder_model = CountVectorizer(analyzer=arguments['analyzer'], ngram_range=arguments['range'], max_features=arguments['max_features'])\n",
    "                encoder_model.fit(train_df['text'])\n",
    "        \n",
    "        train_set = CodemixDataset(train_df, encoder_model, arguments['type'],dataset_name,mode='train')\n",
    "        val_set = CodemixDataset(val_df, encoder_model, arguments['type'],dataset_name,mode='val')\n",
    "        train_loader = torch.utils.data.DataLoader(train_set, batch_size=BATCH_SIZE, shuffle=True)\n",
    "        test_loader = torch.utils.data.DataLoader(val_set, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "        for hiddenNode_idx, num_layers in enumerate([2,4]):\n",
    "            mlp_hiddenNodes = mlp_hiddenNodes_combinations[hiddenNode_idx]\n",
    "            for dp in mlp_dropout: #Dropout\n",
    "                for hn in mlp_hiddenNodes: #hidden nodes\n",
    "\n",
    "                    fname = arguments['model_name']+\"_\"+tag[0]+\"_\"+tag[2]+\"_\"+str(num_layers)+\"L_\"+str(dp)+\"D_\"+str(hn)\n",
    "                    print(fname)\n",
    "\n",
    "                    if num_layers == 2:\n",
    "                        LinearNetwork = LinearNetwork_2Layer\n",
    "                    elif num_layers == 4:\n",
    "                        LinearNetwork = LinearNetwork_4Layer\n",
    "\n",
    "                    #change project name    #entity - nnproj\n",
    "                    run = wandb.init(project=dataset_name, entity=\"nnproj\",reinit=True,name=fname)\n",
    "\n",
    "                    train(LinearNetwork, encoder_model, arguments['type'], train_df, val_df, epochs=EPOCHS, \\\n",
    "                        learning_rate=ETA, batch_size=BATCH_SIZE, log=wandb.log, device=tdevice,hiddenNodes=hn,dropout=dp, train_loader=train_loader, test_loader=test_loader)            \n",
    "\n",
    "                    run.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KTGPGIr-fkLL"
   },
   "source": [
    "# Observation and results"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Final_Project.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
